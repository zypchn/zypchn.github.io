---
title: Perceptron
date: 2025-02-08
categories: [Computational Neuroscience]
tags: [perceptron]
description: Tek katmanlÄ± Perceptron modeline giriÅŸ
toc: false
---


# Biyolojik NÃ¶ronlardan Yapay Sinir AÄŸlarÄ±na : Perceptron

Merhaba!
[Ã–nceki yazÄ±mÄ±n](https://zypchn.github.io/posts/intro-to-brain-science/) devamÄ± olan bu yazÄ±da, biyolojik nÃ¶ronlardan ilham alÄ±narak tasarlanan Perceptron modelinden bahsedeceÄŸim. Bu yazÄ±dan sonra hesaplamalÄ± nÃ¶robilime giriÅŸ yapacaÄŸÄ±mÄ±z iÃ§in Ã¶ncesinde yapay sinir aÄŸlarÄ±nÄ±n temel birimi hakkÄ±nda bilgi sahibi olunmasÄ± gerektiÄŸini dÃ¼ÅŸÃ¼nÃ¼yorum.
Keyifli okumalar!

<br/>

## Perceptron Nedir?
Perceptron, tek katmanlÄ± bir yapay sinir aÄŸÄ±nÄ±n temel birimi olan denetimli bir Ã¶ÄŸrenme algoritmasÄ±dÄ±r. Ä°kili sÄ±nÄ±flandÄ±rma iÃ§in uygundur, yani giriÅŸ deÄŸerlerini iki kategoriye ayÄ±rÄ±r: 0/1 veya -1/1 (kaynaklarda farklÄ±lÄ±k gÃ¶stermektedir).

Perceptron algoritmasÄ± ikiye ayrÄ±lÄ±r:

 1. **Tek KatmanlÄ± Perceptron** : Verinin tek bir Ã§izgiyle bÃ¶lÃ¼nebildiÄŸi senaryolar iÃ§in uygundur. KarmaÅŸÄ±k problemler iÃ§in uygun deÄŸildir. AÅŸaÄŸÄ±da algoritma iÃ§in uygun bir verinin Ã¶rneÄŸi verilmiÅŸtir:
![enter image description here](https://automaticaddison.com/wp-content/uploads/2019/07/linearly-separable.png)

2. **Ã‡ok-KatmanlÄ± Perceptron** : Birden Ã§ok Perceptron katmanÄ± iÃ§erdiÄŸi iÃ§in veri iÅŸlem yeteneÄŸi daha yÃ¼ksektir. Verideki daha karmaÅŸÄ±k iliÅŸkileri Ã§Ã¶zebilir.

Ben bu yazÄ±mda Tek-KatmanlÄ± Perceptron modeli Ã¼zerinden ilerleyeceÄŸim.

<br/>

## Perceptron'un BileÅŸenleri
Bir yapay sinir aÄŸÄ± dÃ¶rt adet bÃ¶lÃ¼mden oluÅŸmaktadÄ±r: giriÅŸ deÄŸerleri (input features), aÄŸÄ±rlÄ±klar ve sapma (weights and bias), aÄŸÄ±rlÄ±klÄ± toplam (weighted sum) ve aktivasyon fonksiyonu (activation function). Bu bileÅŸenleri daha yakÄ±ndan inceleyelim:

**GiriÅŸ DeÄŸerleri :** Her giriÅŸ (input) verisinin bir takÄ±m Ã¶zellikleri vardÄ±r ve bu Ã¶zellikleri temsil etmek iÃ§in sayÄ±sal deÄŸerler atanmÄ±ÅŸtÄ±r. 
[ $x_1 = 3.8, x_2 = 5.1,  ..., x_n = 2.9$ ]
Buradaki her $x_n$ deÄŸeri bir giriÅŸ deÄŸeri olarak adlandÄ±rÄ±lÄ±r. GiriÅŸ deÄŸerlerinin sayÄ±sÄ± verideki Ã¶zelliklerin sayÄ±sÄ±na baÄŸlÄ±dÄ±r.

**AÄŸÄ±rlÄ±klar ve Sapma :** GiriÅŸ verisindeki her bir Ã¶zelliÄŸin Ã§Ä±kÄ±ÅŸ verisi Ã¼zerindeki etkisini belirleyen sayÄ±sal deÄŸerlere *aÄŸÄ±rlÄ±k* denmektedir.  
[ $w_1=2, w_2=8.5, ...,  w_n = -1.4$ ] 
AÄŸÄ±rlÄ±klar, algoritmanÄ±n eÄŸitim sÃ¼resi boyunca deÄŸiÅŸtirilerek optimal deÄŸerleri bulunmaya Ã§alÄ±ÅŸÄ±lÄ±r.
*Sapma* deÄŸeri ise algoritmanÄ±n giriÅŸ deÄŸerlerinden baÄŸÄ±msÄ±z deÄŸiÅŸiklikler yapmasÄ±na olanak tanÄ±yan skalar deÄŸerdir. Genelde $b$ harfiyle (bias) ifade edilir.

**AÄŸÄ±rlÄ±klÄ± Toplam :** AÄŸÄ±rlÄ±klÄ± toplam deÄŸeri, her giriÅŸ deÄŸerinin kendisine ait aÄŸÄ±rlÄ±k deÄŸeriyle Ã§arpÄ±lmasÄ± ve sonuÃ§larÄ±n toplanmasÄ±yla elde edilen deÄŸerdir. 
$z = w_1x_1 + w_2x_2 + w_3x_3 + ..., w_nx_n$

**Aktivasyon Fonksiyonu :** AÄŸÄ±rlÄ±klÄ± toplamda elde edilen deÄŸeri girdi olarak alÄ±r ve Ã§Ä±kÄ±ÅŸ deÄŸerini verir. 
Aktivasyon fonksiyonu problemden probleme veya algoritmadan algoritmaya deÄŸiÅŸiklik gÃ¶sterebilir. Perceptronlar, *Heaviside Basamak Fonksiyonu'nu (binary step)* kullanmaktadÄ±r ve ÅŸÃ¶yle tanÄ±mlanmÄ±ÅŸtÄ±r: 
$$0 \quad if \quad z < Threshold \brace 
1 \quad if \quad z \geq Threshold$$

Buradaki *z* aÄŸÄ±rlÄ±klÄ± toplamÄ±, *Threshold* ise eÅŸik deÄŸerini ifade etmektedir. EÅŸik deÄŸer probleme gÃ¶re deÄŸiÅŸiklik gÃ¶sterebilir.

Perceptron modelinin bileÅŸenlerini Ã¶ÄŸrendik, bir de gÃ¶rsel olarak nasÄ±l modellendiÄŸine bakalÄ±m:
![perceptron](https://media.geeksforgeeks.org/wp-content/uploads/20221219111343/Single-Layer-Perceptron.png)

TasarÄ±m size de tanÄ±dÄ±k gelmedi mi? Sanki ÅŸuna benziyor:
![neuron](https://doktorfizik.com/wp-content/uploads/2019/12/n%C3%B6ron.png)

Ä°ki gÃ¶rseli birleÅŸtirecek olursak: <br/>
![neuron + perceptron](https://miro.medium.com/v2/resize:fit:828/format:webp/1*FLoEcD4bWRw6Zno32uFwuw.png)

GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi Perceptron ve biyolojik nÃ¶ronlar tasarÄ±m olarak oldukÃ§a benziyor. 

<br/>

## Perceptron NasÄ±l Ã‡alÄ±ÅŸÄ±r : Ã–rnek Senaryo
YukarÄ±da anlatÄ±lanlarÄ± pekiÅŸtirmek iÃ§in gerÃ§ek dÃ¼nyadan bir senaryoyla Ã¶rnek verelim. Diyelim ki bir giyim maÄŸazasÄ±ndasÄ±nÄ±z. Ãœzerinde en sevdiÄŸiniz Pokemon'un Ã§izimi olan bir tiÅŸÃ¶rt var ve onu alÄ±p almayacaÄŸÄ±nÄ±za karar vermek istiyorsunuz. Beyniniz *evet* ya da *hayÄ±r* olarak karar verecektir, yani *1* veya *0*. Karar verebilmeniz iÃ§in her Ã¶zelliÄŸin bir deÄŸeri ve aÄŸÄ±rlÄ±ÄŸÄ± olmalÄ±dÄ±r, bunun iÃ§in aÅŸaÄŸÄ±daki tablolara bakalÄ±m:

| GiriÅŸ Ã–zelliÄŸi | GiriÅŸ DeÄŸeri | AÄŸÄ±rlÄ±k |
|--|--|--|
| Fiyat | $x_1 = 400$ | $w_1 = -0.02$ |
| Dizayn | $x_2 = 5$ | $w_2 = 2$ |
| KumaÅŸ Kalitesi | $x_3 = 3.5$ | $w_3 = 1.4$ |

<br/>

| Sapma | EÅŸik DeÄŸer  |
|--|--|
| $b=0.5$ | $T=8$ |


Burada *Dizayn* ve *KumaÅŸ Kalitesi* Ã¶zelliklerine 5 Ã¼zerinden puan verdiÄŸinizi varsayalÄ±m. *Fiyat* ise Ã¼rÃ¼nÃ¼n gerÃ§ek fiyatÄ±. Dikkat ettiyseniz *Fiyat* Ã¶zelliÄŸinin aÄŸÄ±rlÄ±ÄŸÄ± negatif bir deÄŸer. Demek ki fiyatÄ±n yÃ¼ksek olmasÄ± kararÄ±mÄ±zÄ± olumsuz etkiliyor. Kalan Ã¶zelliklerde ise durum tam tersi, yÃ¼ksek puan kararÄ±mÄ±zÄ± olumlu etkiliyor. AÄŸÄ±rlÄ±k deÄŸerleri probleme veya hangi Ã¶zelliÄŸe ne kadar Ã¶nem vermek istediÄŸinize gÃ¶re deÄŸiÅŸebilir.

GiriÅŸ, aÄŸÄ±rlÄ±k, sapma ve eÅŸik deÄŸer tanÄ±mlandÄ±ÄŸÄ±na gÃ¶re aÄŸÄ±rlÄ±klÄ± toplam hesaplamaya geÃ§ebiliriz:
$z = w_1x_1 + w_2x_2 + w_3x_3 + b$
$= (-0.02 \times 400) + (2 \times 5.0) + (1.4 \times 3.5) + 0.5$
$= (-8) + (10) + (4.9) + (0.5) = 7.4$

Daha sonra elde ettiÄŸimiz deÄŸeri $H(z)$ basamak fonksiyonuna verelim:
$$0 \quad if \quad z < 8 \brace 
1 \quad if \quad z \geq 8$$
Buradan, $H(7.4) = 0$ deÄŸeri geliyor.  SonuÃ§ olarak *hayÄ±r* cevabÄ±nÄ± alÄ±yoruz, bye Charizard ğŸ˜­

Burada optimal parametreleri Ã¶nceden bildiÄŸimiz iÃ§in modelimiz direkt karar verebildi. Peki parametreleri nasÄ±l bulduk? Bunun iÃ§in Perceptron'un eÄŸitim sÃ¼recine bakmamÄ±z lazÄ±m.

<br/>

## Perceptron EÄŸitim SÃ¼reci : Kod Ã–rneÄŸi

:warning: Bu bÃ¶lÃ¼mdeki kodlarÄ± anlamak iÃ§in makine Ã¶ÄŸrenme ve PyTorch bilgisi gerekmektedir.

<br/>
Ä°lk Ã¶nce gerekli kÃ¼tÃ¼phaneleri import edelim:

    import torch
    import torch.nn as nn
    from sklearn.datasets import make_blobs
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt
 
 <br/>

Burada hazÄ±r veri seti kullanmak yerine kendi veri setimizi oluÅŸturacaÄŸÄ±z. Bunun iÃ§in *sklearn* kÃ¼tÃ¼phanesinden *make_blobs* fonksiyonunu kullanÄ±yoruz:

    X, y = make_blobs(
    n_samples=1000,		# giriÅŸ verilerinin sayÄ±sÄ±
    n_features=2,		# her giriÅŸ verisinin Ã¶zellik sayÄ±sÄ±
    centers=2,
    cluster_std=3,
    random_state=23
    )
    
    X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.2,			# tÃ¼m verilerin %20'sini test iÃ§in ayÄ±rÄ±yoruz
    random_state=23, 		
    shuffle=True			# verileri bÃ¶lmeden Ã¶nce karÄ±ÅŸtÄ±rÄ±yoruz
    )

<br/>

Veri setimizi oluÅŸturduktan sonra algoritmaya uygun hale getirmek iÃ§in veri Ã¶n iÅŸleme (data preprocessing) yapmamÄ±z gerekiyor:

	# giriÅŸ verilerini sÄ±fÄ±r ortalama ve birim varyansa sahip olacak ÅŸekilde Ã¶lÃ§eklendiriyoruz
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

	# tÃ¼m verileri PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz
	X_train = torch.tensor(X_train, dtype=torch.float32, requires_grad=False)
	X_test = torch.tensor(X_test, dtype=torch.float32, requires_grad=False)
	y_train = torch.tensor(y_train, dtype=torch.float32, requires_grad=False)
	y_train = torch.tensor(y_train, dtype=torch.float32, requires_grad=False)
	
	# y verilerini modelin Ã§Ä±ktÄ± olarak vereceÄŸi tensÃ¶rle aynÄ± formata getiriyoruz
	y_train = y_train.reshape(-1, 1)
	y_test = y_test.reshape(-1, 1)

	torch.manual_seed(42)
	
<br/>

Veri Ã¶n iÅŸleme adÄ±mÄ±mÄ±zÄ± tamamladÄ±k. Åimdi Perceptron sÄ±nÄ±fÄ±mÄ±zÄ± tanÄ±mlayacaÄŸÄ±z:

    class Perceptron(nn.Module):
	    def __init__(self, num_inputs):
		    super(Perceptron, self).__init__()
		    self.linear = nn.Linear(num_inputs, 1)
		
		# Heaviside Basamak Fonksiyonu
		def binary_step_func(self, Z):
			Class = []
			for z in Z:
				if x >= thr:
					Class.append(1)
				else:
					Class.append(0)
			return torch.tensor(Class)
			
		def forward(self, x):
			Z = self.linear(x)
			return self.binary_step_func(Z)

<br/>

Perceptron sÄ±nÄ±fÄ±mÄ±zÄ± oluÅŸturduk. Åimdi objemizi ve maliyet (cost) fonksiyonumuzu tanÄ±mlayalÄ±m:

    perceptron = Perceptron(num_inputs=X_train.shape[1])

	def loss_func(y_pred, Y):
		cost = y_pred - Y
		return cost

<br/>

Åimdi eÄŸitim aÅŸamasÄ±na geÃ§ebiliriz. Bunun iÃ§in *Ã¶ÄŸrenim deÄŸeri (learning rate / step size)* ile *epoch sayÄ±sÄ±* deÄŸerlerini tanÄ±mlayalÄ±m ve ardÄ±ndan eÄŸitim dÃ¶ngÃ¼mÃ¼zÃ¼ yazalÄ±m:

    learning_date = 1e-3 	# 0.001
    num_epochs = 10

	for epoch in range(num_epochs):
		Losses = 0
		for Input, Class in zip(X_train, y_train):
			prediction = perceptron(Input)
			error = loss(prediction, Class)
			Losses += error

			# Perceptron Ã–ÄŸrenme KuralÄ±
		
			# Model Parametreleri
			w = perceptron.linear.weight	# aÄŸÄ±rlÄ±klar
			b = perceptron.linear.bias		# sapma
		
			# GÃ¼ncellenen Model Parametreleri
			w = w - learning_rate * error * Input
			b = b - learning_rate * error

			# GÃ¼ncellenen parametreleri modele geri gÃ¶nderiyoruz
			perceptron.linear.weight = nn.Parameter(w)
			perceptron.linear.bias = nn.Parameter(b)
	  print('Epoch [{}/{}], weight:{}, bias:{} Loss: {:.4f}'.format(
	      epoch+1,num_epochs,
	      w.detach().numpy(),
	      b.detach().numpy(),
	      Losses.item()))

> Epoch [1/10], weight:[[0.6278487 0.24864984]], bias:[-0.08365549] Loss: -82.0000 <br/>
> Epoch [2/10], weight:[[0.6140272 0.10244947]], bias:[-0.05165547] Loss: -32.0000 <br/>
> Epoch [3/10], weight:[[0.5870175 0.0165318]], bias:[-0.0316555] Loss: -20.0000 <br/>
> Epoch [4/10], weight:[[0.558535 -0.04593931]], bias:[-0.0236555] Loss: -8.0000 <br/>
> Epoch [5/10], weight:[[0.5315978 -0.08121765]], bias:[-0.0216555] Loss: -2.0000 <br/>
> Epoch [6/10], weight:[[0.5053688 -0.11094457]], bias:[-0.0216555] Loss: 0.0000 <br/>
> Epoch [7/10], weight:[[0.48110715 -0.1318747 ]], bias:[-0.0186555] Loss: -3.0000 <br/>
> Epoch [8/10], weight:[[0.45835102 -0.14691873]], bias:[-0.0166555] Loss: -2.0000 <br/>
> Epoch [9/10], weight:[[0.43633538 -0.15988223]], bias:[-0.0126555] Loss: -4.0000 <br/>
> Epoch [10/10], weight:[[0.41519952 -0.17041822]], bias:[-0.0106555] Loss: -2.0000 <br/>


Burada tÃ¼m epoch'lar iÃ§in model aÄŸÄ±rlÄ±klarÄ± (weight), sapma (bias) ve kayÄ±p (loss) deÄŸerlerini gÃ¶rebilirsiniz. (Veri seti ve parametreler rastgele oluÅŸturulduÄŸu iÃ§in alacaÄŸÄ±nÄ±z Ã§Ä±ktÄ± farklÄ±lÄ±k gÃ¶sterebilir.)

Modelimiz eÄŸitildiÄŸine gÃ¶re test aÅŸamasÄ±na geÃ§ebiliriz. HatÄ±rlarsanÄ±z en baÅŸta veri setimizin %20'sini ayÄ±rmÄ±ÅŸtÄ±k. Åimdi onu kullanacaÄŸÄ±z:

    pred = perceptron(X_test)	# tahmin edilen deÄŸerler
	
	accuracy = (pred == y_test[:, 0]).float().mean()	# doÄŸruluk metriÄŸi
	print("Test Verisinin DoÄŸruluk DeÄŸeri:", accuracy.item())

> Test Verisinin DoÄŸruluk DeÄŸeri: 0.9599999785423279

DoÄŸruluk deÄŸeri yine parametre ve verilerin rassallÄ±ÄŸÄ±ndan dolayÄ± sizde farklÄ±lÄ±k gÃ¶sterebilir.

<br/>

OkuduÄŸunuz iÃ§in teÅŸekkÃ¼r ederim ğŸ¤“ <br/>
Bir sonraki yazÄ±mda biyolojik nÃ¶ronlarÄ± daha gerÃ§ekÃ§i bir ÅŸekilde uygulamaya koyan modellerden bahsedeceÄŸim. <br/>
GÃ¶rÃ¼ÅŸmek Ã¼zere ğŸ‘‹

<br/>
<br/>

### Kaynaklar :
- https://www.geeksforgeeks.org/what-is-perceptron-the-simplest-artificial-neural-network/
